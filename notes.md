### aitextgen initialization params
:param model: Either the file path of a PyTorch GPT-2 model, or a string
representing the Huggingface model to download.
:param config: Either a file path of a config.json representing the model,
or a GPT2Config with the model architecture.
:param vocab_file: Path to a vocab file (generated by train_tokenizer())
:param merges_file: Path to a merges file (generated by train_tokenizer())
:param cache_dir: folder path which downloaded models will be stored and loaded
:param tf_gpt2: model indicator of OpenAI-distributed version of GPT-2.
This will convert the model to PyTorch if not present.
:param to_gpu: Whether to load the model into the GPU after loading
(good for generation)
:param to_fp16: Whether to convert the model to FP16 before loading
to GPU (for supported GPUs only)
:param verbose: Whether to enable logging from base Huggingface packages
:param bos_token: String to override the beginning-of-string token
:param eos_token: String to override the end-of-string token
:param unk_token: String to override the unknown token


### aitextgen generator params

n: int = 1,
prompt: str = "",
min_length: int = None,
max_length: int = 256,
temperature: float = 0.7,
do_sample: bool = True,
return_as_list: bool = False,
seed: int = None,
pad_token_id: str = None,

:param n: Numbers of texts to generate.
:param prompt: Text to force the generated text to start with
:param max_length: Maximum length for the generated text
:param temperature: Determines the "creativity" of the generated text.
The value range is different for each type of Transformer.
:param do_sample: Samples the text, which is what we want. If False,
the generated text will be the optimal prediction at each time,
and therefore deterministic.
:param return_as_list: Boolean which determine if text should be returned
as a list. If False, the generated texts will be print to console.
:param seed: A numeric seed which sets all randomness, allowing the
generate text to be reproducible if rerunning with same parameters
and model.

Max seed is 2^32-1 (4,294,967,296)